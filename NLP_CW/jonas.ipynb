{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Using cached contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Using cached textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting pyahocorasick\n",
      "  Using cached pyahocorasick-1.4.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (109 kB)\n",
      "Collecting anyascii\n",
      "  Using cached anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/opt/miniconda3/lib/python3.9/site-packages/ahocorasick.cpython-39-x86_64-linux-gnu.so'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dpm_preprocessing import DPMProprocessed\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "model_name = 'roberta'\n",
    "model_path = f'./models/pcl_{model_name}_finetuned/model/'\n",
    "tokenizer_path = f'./models/pcl_{model_name}_finetuned/tokenizer/'\n",
    "MAX_SEQ_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, input_set):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = list(input_set['text'])\n",
    "        self.labels = list(input_set['label'])\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "\n",
    "        texts = []\n",
    "        labels = []\n",
    "\n",
    "        for b in batch:\n",
    "            texts.append(b['text'])\n",
    "            labels.append(b['label'])\n",
    "\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        encodings['labels'] =  torch.tensor(labels)\n",
    "        return encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        item = {'text': self.texts[idx],\n",
    "                'label': self.labels[idx]}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map of label to numerical label:\n",
      "{'Unbalanced_power_relations': 0, 'Shallow_solution': 1, 'Presupposition': 2, 'Authority_voice': 3, 'Metaphors': 4, 'Compassion': 5, 'The_poorer_the_merrier': 6}\n",
      "      par_id      art_id     keyword country  \\\n",
      "0          1  @@24942188    hopeless      ph   \n",
      "1          2  @@21968160     migrant      gh   \n",
      "2          3  @@16584954   immigrant      ie   \n",
      "3          4   @@7811231    disabled      nz   \n",
      "4          5   @@1494111     refugee      ca   \n",
      "...      ...         ...         ...     ...   \n",
      "10464  10465  @@14297363       women      lk   \n",
      "10465  10466  @@70091353  vulnerable      ph   \n",
      "10466  10467  @@20282330     in-need      ng   \n",
      "10467  10468  @@16753236    hopeless      in   \n",
      "10468  10469  @@16779383    homeless      ie   \n",
      "\n",
      "                                                    text  label orig_label  \\\n",
      "0      We are living in times of absolute insanity , ...      0          0   \n",
      "1      In Libya today , there are countless number of...      0          0   \n",
      "2       White House press secretary Sean Spicer said ...      0          0   \n",
      "3      Council customers only signs would be displaye...      0          0   \n",
      "4       Just like we received migrants fleeing El Sal...      0          0   \n",
      "...                                                  ...    ...        ...   \n",
      "10464   Sri Lankan norms and culture inhibit women fr...      0          1   \n",
      "10465  He added that the AFP will continue to bank on...      0          0   \n",
      "10466   She has one huge platform , and information c...      1          3   \n",
      "10467   Anja Ringgren Loven I ca n't find a word to d...      1          4   \n",
      "10468   Guinness World Record of 540lbs of 7 layer mu...      1          3   \n",
      "\n",
      "       lenght  \n",
      "0         123  \n",
      "1          41  \n",
      "2          25  \n",
      "3          30  \n",
      "4          51  \n",
      "...       ...  \n",
      "10464      62  \n",
      "10465      42  \n",
      "10466      54  \n",
      "10467     103  \n",
      "10468      29  \n",
      "\n",
      "[10469 rows x 8 columns]\n",
      "Training set length:  8375\n",
      "Validation set length:  2094\n"
     ]
    }
   ],
   "source": [
    "dpm_pp = DPMProprocessed('.', 'task4_test.tsv')\n",
    "train_df, val_df = dpm_pp.get_unbalanced_split(0.2)\n",
    "\n",
    "print(\"Training set length: \",len(train_df))\n",
    "print(\"Validation set length: \",len(val_df))\n",
    "\n",
    "train_dataset = PCLDataset(tokenizer, train_df)\n",
    "eval_dataset = PCLDataset(tokenizer, val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 10.0]).to(device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        if len(logits) != len(labels):\n",
    "            print(1)\n",
    "        return ((loss, outputs) if return_outputs else loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/opt/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8375\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2094\n",
      "  5%|▍         | 100/2094 [00:13<04:26,  7.49it/s]***** Running Evaluation *****\n",
      "  Num examples = 2094\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7189, 'learning_rate': 9.522445081184336e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  5%|▍         | 101/2094 [00:19<52:35,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6762136816978455, 'eval_f1_macro': 0.0, 'eval_runtime': 5.9298, 'eval_samples_per_second': 353.133, 'eval_steps_per_second': 29.512, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 200/2094 [00:30<03:51,  8.19it/s]***** Running Evaluation *****\n",
      "  Num examples = 2094\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7153, 'learning_rate': 9.044890162368673e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|▉         | 201/2094 [00:36<51:16,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6986951231956482, 'eval_f1_macro': 0.17357174007849976, 'eval_runtime': 5.7691, 'eval_samples_per_second': 362.971, 'eval_steps_per_second': 30.334, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 300/2094 [00:48<03:21,  8.91it/s]***** Running Evaluation *****\n",
      "  Num examples = 2094\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7158, 'learning_rate': 8.567335243553009e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 14%|█▍        | 301/2094 [00:54<53:48,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6761119961738586, 'eval_f1_macro': 0.0, 'eval_runtime': 5.7891, 'eval_samples_per_second': 361.715, 'eval_steps_per_second': 30.229, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 400/2094 [01:06<03:11,  8.85it/s]***** Running Evaluation *****\n",
      "  Num examples = 2094\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7119, 'learning_rate': 8.089780324737345e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 19%|█▉        | 401/2094 [01:12<34:48,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.684873104095459, 'eval_f1_macro': 0.0, 'eval_runtime': 5.7854, 'eval_samples_per_second': 361.946, 'eval_steps_per_second': 30.249, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 500/2094 [01:23<02:52,  9.25it/s]***** Running Evaluation *****\n",
      "  Num examples = 2094\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.684, 'learning_rate': 7.612225405921681e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 24%|██▍       | 500/2094 [01:29<02:52,  9.25it/s]Saving model checkpoint to ./experiment/pcl/checkpoint-500\n",
      "Configuration saved in ./experiment/pcl/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6759063601493835, 'eval_f1_macro': 0.0, 'eval_runtime': 5.7746, 'eval_samples_per_second': 362.625, 'eval_steps_per_second': 30.305, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./experiment/pcl/checkpoint-500/pytorch_model.bin\n",
      " 29%|██▊       | 600/2094 [01:43<02:51,  8.70it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 2094\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7013, 'learning_rate': 7.134670487106018e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=6'>7</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=7'>8</a>\u001b[0m         output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./experiment/pcl\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=8'>9</a>\u001b[0m         learning_rate \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=13'>14</a>\u001b[0m         evaluation_strategy\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=14'>15</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=16'>17</a>\u001b[0m trainer \u001b[39m=\u001b[39m CustomTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=17'>18</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,                         \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=18'>19</a>\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,                 \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=22'>23</a>\u001b[0m         eval_dataset \u001b[39m=\u001b[39m eval_dataset\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=23'>24</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000008?line=24'>25</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py:1440\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1436'>1437</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1437'>1438</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1439'>1440</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1440'>1441</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1441'>1442</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py:1565\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1562'>1563</a>\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1563'>1564</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1564'>1565</a>\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1565'>1566</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=1567'>1568</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py:2208\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2204'>2205</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2206'>2207</a>\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2207'>2208</a>\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2208'>2209</a>\u001b[0m     eval_dataloader,\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2209'>2210</a>\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2210'>2211</a>\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2211'>2212</a>\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2212'>2213</a>\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2213'>2214</a>\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2214'>2215</a>\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2215'>2216</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2217'>2218</a>\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2218'>2219</a>\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2219'>2220</a>\u001b[0m     speed_metrics(\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2220'>2221</a>\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2224'>2225</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2225'>2226</a>\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py:2382\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2378'>2379</a>\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2380'>2381</a>\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2381'>2382</a>\u001b[0m loss, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2383'>2384</a>\u001b[0m \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2384'>2385</a>\u001b[0m     xm\u001b[39m.\u001b[39mmark_step()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py:2590\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2587'>2588</a>\u001b[0m \u001b[39mif\u001b[39;00m has_labels:\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2588'>2589</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2589'>2590</a>\u001b[0m         loss, outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2590'>2591</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/transformers/trainer.py?line=2592'>2593</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n",
      "\u001b[1;32m/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb Cell 5'\u001b[0m in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000007?line=4'>5</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000007?line=5'>6</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000007?line=7'>8</a>\u001b[0m loss_fct \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(weight\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1.0\u001b[39;49m, \u001b[39m10.0\u001b[39;49m])\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000007?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fct(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_labels), labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quenouille/friends/NLP_CW/NLP_CW/jonas.ipynb#ch0000007?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(logits) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(labels):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_loader = DataLoader(eval_dataset)\n",
    "def compute_metric_eval(arg):\n",
    "    logits, labels_gold = arg[0], arg[1]\n",
    "    labels_pred = np.argmax(logits, axis = 1)\n",
    "    return {'f1_macro' :f1_score(labels_gold, labels_pred, average='macro') } #more metrics can be added\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir='./experiment/pcl',\n",
    "        learning_rate = 0.0001,\n",
    "        logging_steps= 100,\n",
    "        per_device_train_batch_size=12,\n",
    "        per_device_eval_batch_size = 12,\n",
    "        num_train_epochs = 3,\n",
    "        evaluation_strategy= \"steps\"\n",
    "        )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "        model=model,                         \n",
    "        args=training_args,                 \n",
    "        train_dataset=train_dataset,                   \n",
    "        data_collator=eval_dataset.collate_fn,\n",
    "        compute_metrics = compute_metric_eval,\n",
    "        eval_dataset = eval_dataset\n",
    "    )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "train_df.to_pickle('train_df.pickle')\n",
    "val_df.to_pickle('val_df.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "train_df = pd.read_pickle('train_df.pickle')\n",
    "val_df = pd.read_pickle('val_df.pickle')\n",
    "\n",
    "train_dataset = PCLDataset(tokenizer, train_df)\n",
    "eval_dataset = PCLDataset(tokenizer, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pcl(input, tokenizer, model): \n",
    "  model.eval()\n",
    "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "  encodings = encodings.to(device)\n",
    "  output = model(**encodings)\n",
    "  logits = output.logits\n",
    "  preds = torch.max(logits, 1)\n",
    "\n",
    "  return {'prediction':preds[1], 'confidence':preds[0]}\n",
    "\n",
    "def evaluate(model, tokenizer, data_loader):\n",
    "\n",
    "  preds = []\n",
    "  tot_labels = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in (data_loader): \n",
    "\n",
    "      labels = {}\n",
    "      labels['label'] = data['label']\n",
    "\n",
    "      tweets = data['text']\n",
    "\n",
    "      pred = predict_pcl(tweets, tokenizer, model)\n",
    "\n",
    "      preds.append(np.array(pred['prediction'].cpu()))\n",
    "      tot_labels.append(np.array(labels['label'].cpu()))\n",
    "\n",
    "  # with the saved predictions and labels we can compute accuracy, precision, recall and f1-score\n",
    "  \n",
    "\n",
    "  return preds, tot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = DataLoader(eval_dataset)\n",
    "\n",
    "preds, tot_labels = evaluate(model, tokenizer, validation_loader)\n",
    "tot_labels = np.array(tot_labels)\n",
    "preds = np.array(preds)\n",
    "report = classification_report(tot_labels, preds, target_names=[\"Not PCL\",\"PCL\"], output_dict= True)\n",
    "print(report)\n",
    "\n",
    "print(report['accuracy'])\n",
    "print(report['Not PCL']['f1-score'])\n",
    "print(report['PCL']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
